{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb41d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from utils.DataPreparation import prepare_data\n",
    "from utils.DataPreparation import scale_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09cbf42",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69f50ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_file = None  # INSERT FILE PATH TO THE DATASET HERE\n",
    "output_file_path = None # INSERT FILE PATH WHERE TO STORE THE PREPARED DATA\n",
    "\n",
    "data      = dd.read_hdf(hdf5_file, 'data', chunksize=14400, mode='r')\n",
    "metadata  = pd.read_hdf(hdf5_file, 'metadata')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7658ff96",
   "metadata": {},
   "source": [
    "## Prepare Data Samples for Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b91f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_h5(path, dataset_name, data_to_append):\n",
    "    try:\n",
    "        with h5py.File(path, 'a') as h5_file:\n",
    "            dataset = h5_file[dataset_name]\n",
    "            current_shape = dataset.shape\n",
    "            new_shape = (current_shape[0] + data_to_append.shape[0],) + current_shape[1:]\n",
    "            dataset.resize(new_shape)\n",
    "            new_slice = (slice(current_shape[0], new_shape[0]),) + (slice(None),) * (len(new_shape) - 1)\n",
    "            dataset[new_slice] = data_to_append\n",
    "    except KeyError:\n",
    "        # Create new dataset if it doesn't exist yet\n",
    "        with h5py.File(path, 'a') as h5_file:\n",
    "            h5_file.create_dataset(dataset_name, data=data_to_append, chunks=True, maxshape=(None,) + data_to_append.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7eb238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_meta_features(meta_features):\n",
    "    '''\n",
    "    :param meta_features: feature array [_volume, infiltration, maxOccupants]\n",
    "    '''\n",
    "    meta_features[0] = scale_data(meta_features[0], min_domain=9.6, max_domain=400)\n",
    "    meta_features[1] = scale_data(meta_features[1], min_domain=0.000085, max_domain=0.00085)\n",
    "    meta_features[2] = scale_data(meta_features[2], min_domain=1, max_domain=12)\n",
    "    return meta_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666f5392",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "validationRooms = 1000 # number of rooms used for validation\n",
    "x_train = None\n",
    "x_val = None\n",
    "n_simulations = len(metadata['simID'].unique())\n",
    "print(\"data from {} simulations are prepared...\".format(n_simulations))\n",
    "\n",
    "for i in range(0, n_simulations):\n",
    "    \n",
    "    # prepare timeseries\n",
    "    df = data[data['simID'] == i]\n",
    "   \n",
    "    x_part, y_part = prepare_data(data.partitions[i]['Zone Air CO2 Concentration'].values.compute(), \n",
    "                                  data.partitions[i]['BinaryOccupancy'].values.compute(),\n",
    "                                  window_size=30, \n",
    "                                  max_batch_size=128,\n",
    "                                  normalize='CO2')\n",
    "    \n",
    "    # prepare metadata\n",
    "    df_meta = metadata[metadata['simID'] == i]\n",
    "    meta_features = scale_meta_features(df_meta[['_volume', 'infiltration', 'maxOccupants']].values[0])\n",
    "    x_meta_part = np.array([meta_features for d in range(0, len(x_part))])\n",
    "    \n",
    "    print(\"saving to file...\")\n",
    "    # save to file\n",
    "    if i < n_simulations - validationRooms:\n",
    "        append_to_h5(output_file_path, 'x_train_timeseries', x_part.astype('float16'))\n",
    "        append_to_h5(output_file_path, 'x_train_metadata', x_meta_part.astype('float16'))\n",
    "        append_to_h5(output_file_path, 'y_train', y_part.astype('float16'))\n",
    "    else:\n",
    "        append_to_h5(output_file_path, 'x_val_timeseries', x_part.astype('float16'))\n",
    "        append_to_h5(output_file_path, 'x_val_metadata', x_meta_part.astype('float16'))\n",
    "        append_to_h5(output_file_path, 'y_val', y_part.astype('float16'))\n",
    "            \n",
    "    print(\"prepared data from {} simulations\".format(i+1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47de23ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(output_file_path, 'r') as h5_file:\n",
    "    print(h5_file['x_train_timeseries'])\n",
    "    print(h5_file['x_train_metadata'])\n",
    "    print(h5_file['y_train'])\n",
    "    print(h5_file['x_val_timeseries'])\n",
    "    print(h5_file['x_val_metadata'])\n",
    "    print(h5_file['y_val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1eea3f",
   "metadata": {},
   "source": [
    "## Clip Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02da078c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip values to range [0, 1] -> do not allow CO2 values above 5000 ppm\n",
    "\n",
    "def clip(a, min_value, max_value):\n",
    "    return np.where(a < 0.0, min_value, np.where(a > 1.0, max_value, a))\n",
    "\n",
    "def removeExtremeValues(filepath, datasetname):\n",
    "    with h5py.File(filepath, 'r+') as h5_file:\n",
    "        with ProgressBar():\n",
    "            dask_array = da.from_array(h5_file[datasetname], chunks=1000000)\n",
    "            i = 0\n",
    "            for j in dask_array.chunks[0]:\n",
    "                print(i, i+j)\n",
    "                h5_file[datasetname][i:i+j] = clip(dask_array[i:i+j].compute(), 0.0, 1.0)\n",
    "                i += j\n",
    "                \n",
    "removeExtremeValues(output_file_path, 'x_val_timeseries')\n",
    "removeExtremeValues(output_file_path, 'x_train_timeseries')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
