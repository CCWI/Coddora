{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb41d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import dask.array as da\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "from utils.DataPreparation import prepare_data\n",
    "from utils.DataPreparation import scale_data\n",
    "from utils.DataGenerator import Generator\n",
    "\n",
    "from Models.CDBLSTM import CDBLSTM\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cadb710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data directory\n",
    "h5_path = \"./prepared_samples.h5\" # SET CORRECT PATH TO PREPARED TRAINING SAMPLES\n",
    "\n",
    "# Model directory\n",
    "project_dir = \"Full_Model/\" # Choose where to save the trained model\n",
    "os.mkdir(project_dir) if not os.path.exists(project_dir) else None # Create folder if it does not exist yet\n",
    "os.mkdir(project_dir + \"/Model_Checkpoint\") if not os.path.exists(project_dir + \"/Model_Checkpoint\") else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09cbf42",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e3178f",
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_file = h5py.File(h5_path, 'r')\n",
    "\n",
    "x_train      = da.from_array(h5_file['x_train_timeseries'], chunks=100000)\n",
    "x_train_meta = da.from_array(h5_file['x_train_metadata'], chunks=100000)\n",
    "y_train      = da.from_array(h5_file['y_train'], chunks=100000)\n",
    "\n",
    "x_val      = da.from_array(h5_file['x_val_timeseries'], chunks=100000)\n",
    "x_val_meta = da.from_array(h5_file['x_val_metadata'], chunks=100000)\n",
    "y_val      = da.from_array(h5_file['y_val'], chunks=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7171cb",
   "metadata": {},
   "source": [
    "## Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd166a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop incomplete batches according to applied batch size of 512\n",
    "\n",
    "train_data_amount = y_train.shape[0] // 512 * 512\n",
    "val_data_amount = y_val.shape[0] // 512 * 512\n",
    "print(\"training samples: {}, batches: {}\".format(train_data_amount, int(train_data_amount / 512)))\n",
    "print(\"validation samples: {}, batches: {}\".format(val_data_amount, int(val_data_amount / 512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1840ecbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data generators for training process\n",
    "\n",
    "generator_train = Generator(x_train[:train_data_amount], \n",
    "                            x_train_meta[:train_data_amount], \n",
    "                            y_train[:train_data_amount], \n",
    "                            batch_size=512, logfile=project_dir+'/datagenerator.log')\n",
    "generator_val   = Generator(x_val[:val_data_amount], \n",
    "                            x_val_meta[:val_data_amount], \n",
    "                            y_val[:val_data_amount], \n",
    "                            batch_size=512)\n",
    "\n",
    "print(len(generator_train), len(generator_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47885229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model architecture\n",
    "\n",
    "model = CDBLSTM(classes=2, features=1, metafeatures=3, window_size=30, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b187c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply early stopping, checkpointing and logging\n",
    "\n",
    "class LogEndOfTraining(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "    def on_train_end(self, logs=None):\n",
    "        with open(self.filename, 'a') as f:\n",
    "            f.write(str(datetime.now()) + \"  Training completed.\\n\")\n",
    "\n",
    "cb = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', \n",
    "                                        patience=5, min_delta=0.00001,\n",
    "                                        verbose=1, restore_best_weights=True),\n",
    "      tf.keras.callbacks.ModelCheckpoint(project_dir + \"/Model_Checkpoint\",\n",
    "         monitor='val_loss', save_best_only=False, save_weights_only=False, verbose=1),\n",
    "      tf.keras.callbacks.CSVLogger(project_dir + \"/training_log.csv\", append=True),\n",
    "      LogEndOfTraining(project_dir + \"/end_of_training.log\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65c7fde",
   "metadata": {},
   "source": [
    "## Run Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406e57be",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model.fit(generator_train,\n",
    "           validation_data=generator_val,\n",
    "            epochs=100, callbacks=cb, shuffle=True,\n",
    "            workers=3, use_multiprocessing=True)\n",
    "except Exception as e:\n",
    "    logging.basicConfig(filename=project_dir + \"/error.log\", level=logging.INFO, \n",
    "                            format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    logging.error(f\"Training failed: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
