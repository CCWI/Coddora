{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb41d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import dask.array as da\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils.DataPreparation import prepare_data\n",
    "from utils.DataPreparation import scale_data\n",
    "from utils.DataGenerator import Generator\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cadb710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data directory\n",
    "hdf5_file = \"../Data/datasets.h5\" # SET CORRECT PATH TO DATASET FILE\n",
    "\n",
    "# Base model directory\n",
    "base_model = '../Models/Coddora' # SET CORRECT PATH TO CODDORA MODEL\n",
    "\n",
    "# Resulting model directory\n",
    "project_dir = '../Models/CoddoraRW/'  # Choose where to save the retrained model\n",
    "os.mkdir(project_dir) if not os.path.exists(project_dir) else None # Create folder if it does not exist yet\n",
    "os.mkdir(project_dir + '/Model_Checkpoint') if not os.path.exists(project_dir + '/Model_Checkpoint') else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d084c4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 'Home' # key for the Home dataset\n",
    "df = pd.read_hdf(hdf5_file, k)\n",
    "print(\"{} ({} days)\".format(k, len(df.Day.unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0023a324",
   "metadata": {},
   "source": [
    "## Define Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1180fdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    \"Office_A\" : {'volume': 77.5,  'infiltration': 0.0001,   'occupants':  2},\n",
    "    \"Home\" :     {'volume':   40,  'infiltration': 0.0001,   'occupants':  2},\n",
    "    \"Stjelja\" :  {'volume': 73.5,  'infiltration': 0.0001,   'occupants': 12},\n",
    "} \n",
    "\n",
    "def scale_meta_features(meta_features):\n",
    "    '''\n",
    "    :param meta_features: feature array [_volume, infiltration, maxOccupants]\n",
    "    '''\n",
    "    meta_features[0] = scale_data(meta_features[0], min_domain=9.6, max_domain=400)\n",
    "    meta_features[1] = scale_data(meta_features[1], min_domain=0.000085, max_domain=0.00085)\n",
    "    meta_features[2] = scale_data(meta_features[2], min_domain=1, max_domain=12)\n",
    "    return meta_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475547b2",
   "metadata": {},
   "source": [
    "## Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a872d605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(k, days, add_metadata=False):\n",
    "    \n",
    "    df = pd.read_hdf(hdf5_file, k)\n",
    "    training_data = df[df.Day.isin(df.Day.unique()[:days])]\n",
    "    val_data     = df[~df.Day.isin(df.Day.unique()[:days])]\n",
    "    print(len(training_data), len(val_data))\n",
    "\n",
    "    x_train, y_train = prepare_data(training_data['CO2'].values, training_data['Occupancy'].values, \n",
    "                                  window_size=30, normalize='CO2', verbose=0)\n",
    "    x_val, y_val = prepare_data(val_data['CO2'].values, val_data['Occupancy'].values, \n",
    "                                  window_size=30, normalize='CO2', verbose=0)\n",
    "    if add_metadata:\n",
    "        meta_features = scale_meta_features(list(metadata[k].values()))\n",
    "        x_train_meta = np.array([meta_features for d in range(0, len(x_train))])\n",
    "        x_val_meta = np.array([meta_features for d in range(0, len(x_val))])\n",
    "        print(\"train\", np.shape(x_train), np.shape(x_train_meta), np.shape(y_train))\n",
    "        print(\"val \", np.shape(x_val), np.shape(x_val_meta), np.shape(y_val))\n",
    "        data = {'x_train': [x_train, x_train_meta], \n",
    "                'y_train': y_train,\n",
    "                'x_val': [x_val, x_val_meta],\n",
    "                'y_val': y_val}\n",
    "    else:\n",
    "        print(\"train\", np.shape(x_train), np.shape(y_train))\n",
    "        print(\"val \", np.shape(x_val), np.shape(y_val))\n",
    "        data = {'x_train': x_train, 'y_train': y_train, 'x_val': x_val, 'y_val': y_val}\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08594ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare 45 days for training, 5 days for validation\n",
    "\n",
    "data = sample_data('Home', 45, add_metadata=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7171cb",
   "metadata": {},
   "source": [
    "## Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd166a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop incomplete batches according to applied batch size of 128\n",
    "\n",
    "train_data_amount = data['y_train'].shape[0] // 128 * 128\n",
    "val_data_amount = data['y_val'].shape[0] // 128 * 128\n",
    "print(\"training samples: {}, batches: {}\".format(train_data_amount, int(train_data_amount / 128)))\n",
    "print(\"validation samples: {}, batches: {}\".format(val_data_amount, int(val_data_amount / 128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7722b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train      = data['x_train'][:train_data_amount][0]\n",
    "x_train_meta = data['x_train'][:train_data_amount][1]\n",
    "y_train      = data['y_train'][:train_data_amount]\n",
    "\n",
    "x_val        = data['x_val'][:val_data_amount][0]\n",
    "x_val_meta   = data['x_val'][:val_data_amount][1]\n",
    "y_val        = data['y_val'][:val_data_amount]\n",
    "\n",
    "print(\"training:\",   np.shape(x_train), np.shape(x_train_meta), np.shape(y_train))\n",
    "print(\"validation:\", np.shape(x_val), np.shape(x_val_meta), np.shape(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b187c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply early stopping, checkpointing and logging\n",
    "\n",
    "class LogEndOfTraining(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "    def on_train_end(self, logs=None):\n",
    "        with open(self.filename, 'a') as f:\n",
    "            f.write(str(datetime.now()) + \"  Training completed.\\n\")\n",
    "\n",
    "cb = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', \n",
    "                                        patience=5, min_delta=0.00001,\n",
    "                                        verbose=1, restore_best_weights=True),\n",
    "      tf.keras.callbacks.ModelCheckpoint(project_dir + \"/Model_Checkpoint\",\n",
    "         monitor='val_loss', save_best_only=True, save_weights_only=False, verbose=1),\n",
    "      tf.keras.callbacks.CSVLogger(project_dir + \"/training_log.csv\", append=True),\n",
    "      LogEndOfTraining(project_dir + \"/end_of_training.log\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc09a2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load base model\n",
    "\n",
    "model = tf.keras.models.load_model(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c459ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed value\n",
    "\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65c7fde",
   "metadata": {},
   "source": [
    "## Run Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406e57be",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model.fit([x_train, x_train_meta], y_train, \n",
    "              validation_data=([x_val, x_val_meta], y_val),\n",
    "              epochs=100, batch_size=128, callbacks=cb, shuffle=True)\n",
    "except Exception as e:\n",
    "    logging.basicConfig(filename=project_dir + \"/error.log\", level=logging.INFO, \n",
    "                            format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    logging.error(f\"Training failed: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
